{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gZBh_NgIKrH"
   },
   "source": [
    "# Echantillonnage compressif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqCPWnp9IKrJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.fftpack as fft\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as npl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5e2frcb4IKrP"
   },
   "source": [
    "Récemment (début années 2004-présent), de nouveaux concepts et théorèmes ont été développés et risquent de \n",
    "révolutionner à relativement court terme la fabrication de certains appareils de mesure numériques (microphones, imageurs, analyseurs de spectres,...). \n",
    "Ces nouvelles techniques sont couramment appelées échantillonnage compressif, \"compressive sampling\" ou encore \"compressed sensing\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_J1hYHaIKrR"
   },
   "source": [
    "## 1. Le théorème de Shannon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InF6-4GAIKrS"
   },
   "source": [
    "Aujourd'hui, presque tous les appareils de mesure reposent sur le théorème de Shannon. Celui-ci (vous l'avez déjà vu en 2ème année) peut s'énoncer ainsi : \n",
    "> Soit $g:\\mathbb{R}\\to \\mathbb{R}$ une fonction de $L^2(\\mathbb{R})$. Si sa transformée de Fourier $\\hat g$ a un support contenu dans l'intervalle $[-f_M, f_M]$, alors en l'échantillonnant à une fréquence d'échantillonnage $f_e\\geq 2f_M$, on peut la reconstruire exactement.\n",
    "\n",
    "Les instruments de mesures qui reposent sur ce théorème sont donc construits suivant le principe : \n",
    ">Filtre passe-bas $\\rightarrow$ Echantillonnage à une fréquence $f>2f_M$ $\\rightarrow$ Interpolation sinc\n",
    "\n",
    "Pour beaucoup d'applications, ce principe présente deux défauts majeurs :\n",
    "* Les signaux sont rarement naturellement à spectre borné, et on perd donc l'information haute-fréquence en effectuant un filtrage passe-bas.\n",
    "* Pour beaucoup de signaux, il faut choisir une très haute fréquence d'échantillonnage pour obtenir un résultat satisfaisant. \n",
    "Ceci implique que les données à stocker ont une taille très importante et qu'il faut les compresser après coup (par exemple : jpeg).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdbpDOHcIKrT"
   },
   "source": [
    "## 2. L'échantillonnage compressif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVfD_-GFIKrU"
   },
   "source": [
    "**1. Principe général**\n",
    "\n",
    "L'idée sous jacente à l'échantillonnage compressif est de réaliser la compression dès l'acquisition.\n",
    "Supposons que le signal $x\\in \\mathbb{R}^n$ que l'on souhaite mesurer s'écrive comme une combinaison linéaire de la forme :\n",
    "\\begin{equation}\n",
    "(1)~~~~~~~~~~~ x=\\sum_{i=1}^m\\alpha_i \\psi_i\n",
    "\\end{equation}\n",
    "où $\\psi_i\\in \\mathbb{R}^n, \\ i=1..m$, sont des \"fonctions de base\" (en traitement d'images, ces fonctions pourraient être des ondelettes, en traitement du son, des ondelettes ou des atomes de Fourier, pour certaines applications, on pourrait imaginer des splines...} et $\\alpha_i\\in \\mathbb{R}$ sont des coefficients. \n",
    "On peut réécrire l'équation (1) sous la forme matricielle condensée :\n",
    "$$\n",
    "x=\\Psi \\alpha \\ \\ \\textrm{où } \\ \\ \\alpha=\\begin{pmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_m \\end{pmatrix}\\ \\ \\textrm{et} \\ \\ \\Psi=\\begin{pmatrix} \\psi_1,\\psi_2,..., \\psi_m\\end{pmatrix}.\n",
    "$$\n",
    "Pour pouvoir reconstruire tous les éléments de $\\mathbb{R}^n$, on suppose généralement que la matrice $\\Psi$ est une matrice surjective (ainsi, la famille  des $(\\Psi_i)_i$ est génératrice), ce qui implique que $m\\geq n$. Dans le langage du traitement d'image, on dit alors que $\\Psi$ est un frame (une base si $m=n$).\n",
    "\n",
    "L'échantillonnage compressif repose sur l'hypothèse suivante : les signaux $x$ que l'on souhaite mesurer sont parcimonieux, \n",
    "c'est-à-dire que la majorité des coefficients $\\alpha_i$ dans (1) sont nuls ou encore que \n",
    "$$\\#\\{\\alpha_i\\neq 0, i=1..m\\}\\ll n.$$\n",
    "On va voir que cette hypothèse permet - dans certains cas - de réduire drastiquement le nombre de mesures par rapport au théorème de Shannon avec en contre-partie, le besoin de résoudre un problème d'optimisation pour reconstruire la donnée. L'objectif de ce travail est de résoudre le problème d'optimisation résultant.\n",
    "\n",
    "Le principe de l'acquisition du signal $x$ est le suivant :\n",
    "\n",
    "- On effectue un petit nombre $p\\ll n$ de mesures linéaires du signal $x$ inconnu. On note ces mesures $y_i$, et comme elles sont linéaires par rapport à $x$, il existe pour chaque $i$ un vecteur $a_i\\in \\mathbb{R}^n$ tel que \n",
    "$$y_i=\\langle a_i, x\\rangle, i=1..p.$$ On peut aussi écrire cette opération de mesure sous la forme condensée :\n",
    "$$\n",
    "y=Ax\\ \\ \\textrm{où } \\ \\ y=\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_p\\end{pmatrix} \\ \\ \\textrm{et} \\ \\ A=\\begin{pmatrix} a_1^T\\\\a_2^T\\\\ \\vdots\n",
    "\\\\ a_p^T\\end{pmatrix}.\n",
    "$$\n",
    "- On reconstruit le signal $x$ en résolvant le problème contraint suivant :\n",
    "\n",
    "$$\n",
    "(2)~~~~~~~~~~~ \\mbox{Trouver } \\alpha^\\star \\mbox{ solution de: }\\displaystyle\\min_{\\alpha \\in \\mathbb{R}^m, A\\Psi\\alpha=y} \\|\\alpha\\|_0\n",
    "$$\n",
    "\n",
    "où $\\|\\cdot\\|_0$ est la norme de comptage, aussi appelée norme $l^0$ définie par : \n",
    "$$\n",
    "\\|\\alpha\\|_0=\\#\\{\\alpha_i\\neq 0, i=1..m\\}.\n",
    "$$\n",
    "Autrement dit, l'idée est la suivante : on  cherche $\\alpha^\\star$, le signal le plus parcimonieux dans le frame $\\Psi$, parmi les signaux qui peuvent donner lieu aux mesures $y$. \n",
    "Après avoir trouvé $\\alpha^\\star$, on recouvre $\\tilde x$, une approximation du signal $x$ en calculant $\\tilde x=\\Psi\\alpha^\\star$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "veVopJoKIKrV"
   },
   "source": [
    "**2. Simplification du problème d'optimisation**\n",
    "\n",
    "Le problème précédent est un problème combinatoire NP-complet, ce qui signifie que trouver $\\alpha$ peut demander un temps exponentiel en fonction de $n$, la dimension du signal. Pour le résoudre en pratique, il est souvent remplacé par : \n",
    "\n",
    "$$\n",
    "(3)~~~~~~~~~~~  \\mbox{Trouver } \\alpha^*\\in \\displaystyle\\arg\\min_{\\alpha \\in \\mathbb{R}^m, A\\Psi\\alpha=y} \\|\\alpha\\|_1\n",
    "$$\n",
    "\n",
    "où $\\|\\alpha\\|_1=\\sum_{i=1}^m|\\alpha_i|$ est la norme $l^1$ de $\\alpha$. On peut dans certains cas montrer que les solutions de (2) et de (3) sont identiques. \n",
    "\n",
    "Un appareil de mesure n'étant jamais parfait, il est impossible de mesurer exactement $y_i=\\langle a_i, x\\rangle$. \n",
    "Le vecteur $y$ est bruité et la contrainte $A\\Psi\\alpha=y$ est trop forte. Elle est donc généralement relaxée et le problème devient : \n",
    "\n",
    ">$$(4)~~~~~~~~~~~  \\mbox{Trouver } \\alpha^*\\in \\arg\\min_{\\alpha \\in \\mathbb{R}^m} \\|\\alpha\\|_1 + \\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2.$$\n",
    "\n",
    "Si $\\sigma$ tend vers $0$, la solution du problème (4) tend vers une solution du problème (3). C'est le problème (4) que nous allons résoudre dans ce travail. Dans la suite , on notera $F$ la fonction :\n",
    "$$\n",
    "F(\\alpha)=\\|\\alpha\\|_1 + \\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2.\n",
    "$$\n",
    "\n",
    "Pour conclure cette introduction à l'échantillonnage compressif, notons que de façon similaire au théorème de Shannon, on dispose d'une condition de reconstruction exacte :\n",
    "\n",
    "> Supposons que :\n",
    "* $x=\\displaystyle\\sum_{i=1}^m\\alpha_i\\psi_i\\in \\mathbb{R}^n$ avec $\\|\\alpha\\|_0=k$.\n",
    "* On effectue $p$ mesures linéaires de $x$ avec $p\\geq C \\cdot k \\cdot \\log(n)$, où $C=20$.\n",
    "* On choisit les coefficients de la matrice $A\\in \\mathcal{M}_{p,n}$ de façon **aléatoire** (e.g. on peut choisir les coefficients $a_{i,j}$ de $A$ de façon indépendante suivant une loi normale.)\n",
    "\n",
    "> Alors, la résolution du problème (3) permet de reconstruire $x$ **exactement** avec une très grande probabilité \n",
    "\n",
    "L'expérience a montré qu'en pratique, il suffit en général de $p=2k$ mesures pour reconstruire le signal exactement en grande dimension !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKxFgsr3IKrX"
   },
   "source": [
    "3. Préliminaires théoriques\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TJm8mWLIKrY"
   },
   "source": [
    "Commençons par remarquer que les problèmes (3) et (4) sont convexes (contraintes convexes et fonctions convexes) tandis que le problème (2) ne l'est pas. En revanche, aucun des trois problèmes n'est différentiable.\n",
    "\n",
    "**Q1.** Soit $J(\\alpha) =\\frac{\\sigma}{2}\\|A\\Psi \\alpha - y\\|_2^2$. Calculez $\\nabla J(\\alpha)$.\n",
    "\n",
    "### Réponse: \n",
    "\n",
    "$\\nabla J(\\alpha)$ = $(A\\Psi)^T\\sigma(A\\Psi \\alpha - y)$. \n",
    "\n",
    "\n",
    "**Q2.** Montrer que la fonction $J$ est de classe $C^1$ à gradient Lipschitz et calculer un majorant $L$ de la constante de Lipschitz de $\\nabla J$ en fonction de $|||A|||$, de $|||\\Psi |||$ et de $\\sigma$.\n",
    "\n",
    "\n",
    " ### Réponse: \n",
    "\n",
    "\n",
    "Il s'agit d'une composition entre une application linéaire qu'on peut noter L,tel que L: x -> $A\\Psi x -y$ et une norme $||$ qui est  $C^\\infty$ donc J est $C^\\infty$.\n",
    "\n",
    "$\\|\\nabla J(\\alpha_1)-\\nabla J(\\alpha_2)\\| \\leq \\sigma|||(A\\Psi)^T||| \\|(A\\Psi (\\alpha_1 - \\alpha_2)\\| \\leq 2\\sigma |||A|||^2\\|\\alpha_1-\\alpha_2\\|$. Donc $L=2\\sigma |||A|||^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpXwq8wBIKrZ"
   },
   "source": [
    "**Construction de l'algorithme**\n",
    "\n",
    "On note $\\alpha^k$ l'itéré courant. En appliquant le lemme de Nesterov à la fonction $J$, on a:\n",
    "$$\\forall \\alpha\\in \\mathbb{R}^m,~J(\\alpha)\\leq {J(\\alpha^k) + \\langle \\nabla J(\\alpha^k), \\alpha-\\alpha^k\\rangle + \\frac{L}{2}\\|\\alpha-\\alpha^k\\|_2^2}.\n",
    "$$\n",
    "\n",
    "En posant $\\phi(\\alpha,\\alpha^k)=J(\\alpha^k) + \\langle \\nabla J(\\alpha^k), \\alpha-\\alpha^k\\rangle + \\frac{L}{2}\\|\\alpha-\\alpha^k\\|_2^2 +\\|\\alpha\\|_1$, on a alors:\n",
    "\n",
    "$$\\forall \\alpha\\in \\mathbb{R}^m,~F(\\alpha) = J(\\alpha)+\\|\\alpha\\|_1\\leq \\phi(\\alpha,\\alpha^k),$$\n",
    "\n",
    "avec : $\\phi(\\alpha^k,\\alpha^k) = F(\\alpha^k)$.\n",
    "\n",
    "Cette inégalité motive alors l'algorithme de descente suivant:\n",
    "\n",
    "$$\\alpha^{k+1}=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m} \\phi(\\alpha,\\alpha^k).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hg8Nnn1sIKra"
   },
   "source": [
    "**Q3.** Montrer que l'algorithme s'écrit de façon équivalente sous la forme:\n",
    "$$\\alpha^{k+1} = \\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(\\alpha^k - \\frac{1}{L}\\nabla J(\\alpha^k)\\right).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlxN22T-I7ca"
   },
   "source": [
    "### Réponse: \n",
    "\n",
    "On a que\n",
    "$$\\alpha^{k+1}=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m} \\phi(\\alpha,\\alpha^k)=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m}(J(\\alpha^k) + \\langle \\nabla J(\\alpha^k), \\alpha-\\alpha^k\\rangle + \\frac{L}{2}\\|\\alpha-\\alpha^k\\|_2^2 +\\|\\alpha\\|_1)$$\n",
    "\n",
    "D'autre part on a \n",
    "$$\\begin{align}\n",
    "\\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(\\alpha^k - \\frac{1}{L}\\nabla J(\\alpha^k)\\right)&=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m}(\\frac{1}{L}\\|\\alpha\\|_1+\\frac{1}{2}\\|\\alpha-\\alpha^k+\\frac{1}{L}\\nabla J(\\alpha^k)\\|_2^2)\\\\\n",
    "&=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m}(\\|\\alpha\\|_1+\\frac{L}{2}\\|\n",
    "\\alpha-\\alpha^k\\|_2^2+\\frac{1}{2L}\\|\\nabla J(\\alpha^k)\\|_2^2+\\langle\\alpha-\\alpha^k,\\nabla J(\\alpha^k)\\rangle)\n",
    "\\end{align}$$\n",
    "\n",
    "Comme on a  $\\frac{1}{2L}\\|\\nabla J(\\alpha^k)\\|_2^2$ et $J(\\alpha^k)$ sont indépendant de $\\alpha$, on a bien que \n",
    "\n",
    "\n",
    "$$\\alpha^{k+1}=\\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(\\alpha^k - \\frac{1}{L}\\nabla J(\\alpha^k)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3divgHslI-4L"
   },
   "source": [
    "**Q4.** En déduire la formule analytique donnant $\\alpha^{k+1}$ en fonction de $\\alpha^k$.\n",
    "### Réponse: \n",
    "\n",
    "On en deduit que $$\\begin{align}\n",
    "\\alpha^{k+1}&=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m}(\\frac{1}{L}\\|\\alpha\\|_1+\\frac{1}{2}\\|\\alpha-\\alpha^k+\\frac{1}{L}\\nabla J(\\alpha^k)\\|_2^2)\\\\\n",
    "&=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m}(\\frac{1}{L}\\sum_{i=1}^{m}|\\alpha_i|+\\frac{1}{2}\\sum_{i=1}^{m}(\\alpha_i-\\alpha_i^k+\\frac{1}{L}\\nabla J (\\alpha_k)_i)^2)\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYGgxkyQJA_u"
   },
   "source": [
    "\n",
    "**Q5.** De quelle quantité la fonction coût $F(\\alpha)$ décroît-elle à chaque itération ?\n",
    "\n",
    "### Réponse: \n",
    "\n",
    "$$F(\\alpha^{k+1})-F(\\alpha^k)=J(\\alpha^{k+1})+\\|\\alpha^{k+1}\\|_1-J(\\alpha^{k})+\\|\\alpha^{k}\\|_1\\leq \\phi(\\alpha^{k+1},\\alpha^k)-\\phi(\\alpha^k,\\alpha^k)\\leq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg2usrMZIKrb"
   },
   "source": [
    "4. Partie expérimentale\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jNMKcWPIKrc"
   },
   "source": [
    "Dans ce travail, on va chercher à reconstruire un signal unidimensionnel $x~:~[0,1]~\\rightarrow~\\mathbb{R}$ de la forme :\n",
    "$$\n",
    "x(t)= \\alpha_{k}\\delta_{k/n}(t)+\\sum_{k=1}^n\\alpha_{k+n}\\cos\\left(\\frac{2k\\pi}{n} t\\right)\n",
    "$$ \n",
    "on a donc $m=2n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "36om5voqIKrd",
    "outputId": "563f1773-8b87-4bed-e8bb-5d76d5b51ef3"
   },
   "outputs": [],
   "source": [
    "## Initialisations\n",
    "n=500            #Taille de l'echantillon\n",
    "t=np.linspace(0,1,n) #On definit un signal sur [0,1]\n",
    "\n",
    "## Generation du signal\n",
    "x=np.zeros(n)\n",
    "tmp=np.zeros(n)\n",
    "#On ajoute deux cosinus\n",
    "tmp[350]=4\n",
    "x+=fft.idct(tmp,norm='ortho')  \n",
    "tmp=np.zeros(n)\n",
    "tmp[150]=-3  \n",
    "x+=fft.idct(tmp,norm='ortho')\n",
    "#On ajoute deux diracs\n",
    "x[int(n/3)]=0.2;    #Tester 0.5\n",
    "x[int(2*n/3)]=-0.3; #Tester -1\n",
    "x0=np.copy(x)\n",
    "\n",
    "plt.plot(t,x,'b')\n",
    "plt.show()\n",
    "## Mesure du signal\n",
    "p=20*4       #Nombre de mesures\n",
    "A=np.random.randn(p,n) #La matrice de mesure\n",
    "y=A.dot(x)        #Les mesures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSx-abS4IKrh"
   },
   "source": [
    "Le code *Generesignal.py* génère un signal discret $x$ qui peut être vu comme une combinaison linéaire de cosinus à différentes fréquences et de diracs. Ce signal n'est pas parcimonieux dans la base canonique des diracs (car il faut à peu près $n$ diracs pour représenter un cosinus) et il n'est pas parcimonieux dans la base des sinus (il faut faire une combinaison linéaire de $n$ cosinus pour représenter un dirac).\n",
    "\n",
    "Par contre, ce signal est parcimonieux dans un frame qui est l'union de la base canonique et de la base des cosinus. \n",
    "Dans ce frame, il suffit en effet de $4$ coefficients non nuls pour reconstruire parfaitement le signal.\n",
    "\n",
    "> On choisira donc le frame représenté par une matrice $\\Psi=(I,C) \\in \\mathcal{ M}_{2n,n}(\\mathbb{R})$ o\\`u $C$ est une base de cosinus à différentes fréquences.\n",
    "\n",
    "### 4.1. Implémentation de l'itération proximale\n",
    "\n",
    "**Q6** Implémentez l'opérateur linéaire $\\Psi$ et son adjoint $\\Psi^*$. \n",
    "\n",
    "Pour $\\Psi$, vous vous servirez de la fonction $dct$ de Python dans la libraire scipy.fftpack qui calcule la transformée en cosinus discret d'un vecteur. Vous ferez attention à préciser *norm='ortho'* dans les options de la $dct$ pour que $idct$ soit bien l'opération inverse de $dct$.\n",
    "\n",
    "Pour $\\Psi^*$, vous utiliserez le fait que la $dct$ est une isométrie quand on précise \\textit{norm='ortho'} dans les options de $dct$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHR7CnNtIKri"
   },
   "outputs": [],
   "source": [
    "## Linear function Psi\n",
    "## (combination of sines and diracs)\n",
    " \n",
    "def Psi(alpha) :\n",
    "    n=len(alpha)\n",
    "    x=np.zeros(int(n/2))\n",
    "    x=alpha[:int(n/2)]+fft.idct(alpha[int(n/2):], norm ='ortho')\n",
    "    return x\n",
    "\n",
    "\n",
    "## The transpose of Psi\n",
    "def PsiT(x) : \n",
    "    alpha=np.concatenate((x,fft.dct(x,norm='ortho')),axis=0)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbfpz6G-IKro"
   },
   "source": [
    "**Q7** Implémentez l'algorithme proximal dans la fonction *RestoreX* avec les notations suivantes:\n",
    "* $A$ est la matrice d'échantillonnage.\n",
    "* $y$ est le vecteur de mesures.\n",
    "* $sigma$ est un paramètre du modèle.\n",
    "* $nit$ est le nombre d'itérations.\n",
    "* $alpha$ est la solution approximative du problème (4).\n",
    "* $x$ est donné par Psi $(\\alpha)$.\n",
    "* $CF$ est la fonction coût à chaque itération de l'algorithme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wkSb61kIKrp"
   },
   "outputs": [],
   "source": [
    "def gradJ(u, sigma, A, y) : \n",
    "    return sigma * PsiT( A.T.dot( A.dot( Psi(u) ) - y))\n",
    "\n",
    "def cf(u, sigma, A, y) : \n",
    "    return (sigma/2) * npl.norm(A.dot(Psi(u)) - y )**2 + npl.norm(u,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2uPcPz8JIKrt"
   },
   "outputs": [],
   "source": [
    "## Prox of the l1−norm \n",
    "def prox(alpha,gamma) :\n",
    "    beta=np.sign(alpha)*np.maximum(np.abs(alpha)-gamma,0)\n",
    "    return beta\n",
    "\n",
    "def RestoreX(A,y,sigma,nit) :\n",
    "    n=np.shape(A)[1]\n",
    "    x=np.zeros(nit*n) \n",
    "    alpha=np.zeros(2*n)\n",
    "    CF=np.zeros(nit)\n",
    "    iter=0\n",
    "    while iter<nit:\n",
    "        L=2*sigma*npl.norm(A)**2\n",
    "        alpha=prox(alpha-(1/L)*sigma*PsiT(A.T.dot(A.dot(Psi(alpha))-y)),1/L)\n",
    "        x[n*iter:n*(iter+1)]=Psi(alpha)\n",
    "        CF[iter]=npl.norm(alpha,1)+(sigma/2)*npl.norm(A.dot(Psi(alpha))-y)**2\n",
    "        iter+=1\n",
    "    return (alpha,x,CF)\n",
    "\n",
    "def RestoreX_with_List_alpha(A,y,sigma,nit) :\n",
    "    n=np.shape(A)[1]\n",
    "    x=np.zeros(nit*n) \n",
    "    alpha=np.zeros(2*n)\n",
    "    CF=np.zeros(nit)\n",
    "    Al=[alpha]\n",
    "    iter=0\n",
    "    while iter<nit:\n",
    "        L=2*sigma*npl.norm(A)**2\n",
    "        alpha=prox(alpha-(1/L)*sigma*PsiT(A.T.dot(A.dot(Psi(alpha))-y)),1/L)\n",
    "        Al+=[alpha]\n",
    "        x[n*iter:n*(iter+1)]=Psi(alpha)\n",
    "        CF[iter]=npl.norm(alpha,1)+(sigma/2)*npl.norm(A.dot(Psi(alpha))-y)**2\n",
    "        iter+=1\n",
    "    return (Al,x,CF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8up7EJyKIKrx"
   },
   "source": [
    "**Q8.** Testez votre algorithme ! Les paramètres $sigma$ et $nit$ sont des à choisir par vous-même (il faut en pratique beaucoup d'itérations pour converger). Vous pourrez observer la façon dont la suite $\\alpha^k$ se comporte au fur et à mesure des itérations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVX9_G6t_TaM"
   },
   "source": [
    "Dans cette partie, nous fixons $\\sigma$ à 0.05 et $nit$ à 5000 itérations.\n",
    "Nous commençons d'abord par tester le comportement de la suite $\\alpha^k$ au fur et à mesure des itérations. Pour ce faire, nous décidons de faire des plot de plusieurs itérations espacées de 500. Ce qui nous permet d'obtenir les 11 courbes ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "colab_type": "code",
    "id": "aeUDHTRHIKrx",
    "outputId": "232d690f-bde1-47ec-8836-c0688f19c4a0"
   },
   "outputs": [],
   "source": [
    "sigma=0.05\n",
    "nit=5000\n",
    "(alpha,xtilde,CF)=RestoreX_with_List_alpha(A,y,sigma,nit)\n",
    "fig, (AXE) = plt.subplots(2, 5)\n",
    "fig.suptitle(r\"$\\sigma$ = \"+ \"{:.1e}\".format(sigma) + \" / nit =\" + \"{:}\".format(nit),fontsize=40)\n",
    "fig.set_size_inches(40, 15)\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "Old2New = lambda i : np.array([i//5 , i%5])\n",
    "for i in range(10):\n",
    "    axe = AXE[Old2New(i)[0],Old2New(i)[1]]\n",
    "    axe.set_title(\" iteration = \"+\"{:}\".format(i+1))\n",
    "    axe.plot(alpha[500*i])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76bbbFvK_emW"
   },
   "source": [
    "Nous débutons avec le vecteur $\\alpha$ qui contient que des 0.\n",
    "\n",
    "Après chaque itération, le nombre de coefficient non nul augmente. \n",
    "\n",
    "On peut supposer que l'algorithme converge au vus de l'allure des coefficients de $\\alpha$ sur la dernière itération.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhiMjCro_fEC"
   },
   "source": [
    "Essayons de trouver l'impact de $\\sigma$ et de $nit$ sur la précision du signal reconstruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "auhEdGIVGlu9",
    "outputId": "58d95484-3d6c-45bc-c13f-2a0436e0d1c9"
   },
   "outputs": [],
   "source": [
    "SIG = [0.001,0.01,0.05,0.1,0.15,1]\n",
    "NIT = [500, 1000, 5000, 10000, 20000]\n",
    "#plt.subplots_adjust(hspace=0.8)\n",
    "for sigma in SIG:\n",
    "    fig, (AXE) = plt.subplots(1, len(NIT))\n",
    "    fig.suptitle(r\"$\\sigma$ = \"+ \"{:.1e}\".format(sigma), fontsize=25)\n",
    "    fig.set_size_inches(25, 4)\n",
    "    for i in range(len(NIT)):\n",
    "        nit = NIT[i]\n",
    "        axe = AXE[i]\n",
    "        (alpha,xtilde,CF)=RestoreX(A,y,sigma,nit)\n",
    "        axe.plot(alpha)\n",
    "        axe.set_title(\"nit =\" + \"{:}\".format(nit),y = -0.2,fontsize = 15)\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYvjWy-9_tmC"
   },
   "source": [
    "On remarque que pour de trop petites valeurs de $\\sigma$ comme  1e-3, l'algorithme ne converge pas quelque soit $nit$ (malgré le fait que si $\\sigma$ tend vers 0, la solution du problème (4) tend vers une solution du problème (3)).\n",
    "\n",
    "A partir de $\\sigma$  supérieurs à 1e-2 par exemple, on a une convergence qui se fait que si le nombre d'itération $nit$ est assez élevé et $nit$ doit évoluer avec $\\sigma$ pour avoir convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98glpbN3IKr0"
   },
   "source": [
    "**Q9.** Vérifiez que la fonction coût décroit de façon monotone. Quel est le taux de convergence observé ?\n",
    "\n",
    "Nous reprenons le même test que précedemment mais à la différence, nous allons observer la variation de la fonction coût et ensuite déterminer pour celle-ci le taux de convergence correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "i8zLsySBIKr1",
    "outputId": "888dac64-55fb-4360-8193-edfcf09985a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SIG=[0.001,0.01,0.05,0.1,0.15,1]\n",
    "NIT=[500, 1000, 5000, 10000, 20000]\n",
    "from matplotlib.ticker import FixedLocator, MultipleLocator\n",
    "\n",
    "#plt.subplots_adjust(hspace=0.8)\n",
    "for sigma in SIG:\n",
    "    fig, (AXE) = plt.subplots(1, len(NIT))\n",
    "    fig.suptitle(r\"$\\sigma$ = \"+ \"{:.1e}\".format(sigma), fontsize=25)\n",
    "    fig.set_size_inches(25, 4)\n",
    "    for i in range(len(NIT)):\n",
    "        nit = NIT[i]\n",
    "        axe = AXE[i]\n",
    "        axe.grid()\n",
    "        (alpha,xtilde,CF)=RestoreX(A,y,sigma,nit)\n",
    "        axe.plot(CF)\n",
    "        axe.yaxis.set_major_locator(FixedLocator([min(CF), max(CF)]))\n",
    "        axe.axhline(y=min(CF),color='r',linewidth=0.5)\n",
    "        axe.set_title(\"nit =\" + \"{:}\".format(nit),y=-0.2,fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8bOxNKu_4Oz"
   },
   "source": [
    "On remarque que  la fonction coût est monotone quelque soit les valeurs de $\\sigma$ et de $nit$ .\n",
    "\n",
    "De plus pour les très petites valeurs de $\\sigma$ par exemple 1e-3, la fonction coût n'évolue pas, en effet pour cette valeur de $\\sigma$, on avait le vecteur nul pour $\\alpha$\n",
    "\n",
    "Pour des valeurs de $\\sigma$ de l'ordre de 1e-2, le courbe est décroissante.\n",
    "\n",
    "Expérimentalement, on trouve que $\\sigma = 0.01$ est le meilleur choix pour avoir une bonne convergence\n",
    "\n",
    "Ainsi, dans la suite, nous ferons des tests pour $\\sigma=0.01$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9T0U0Yg_9sT"
   },
   "outputs": [],
   "source": [
    "def Conv_rate(x, x_exact):\n",
    "    e = [abs(x_ - x_exact) for x_ in x]\n",
    "    q = [np.log(e[n+1]/e[n])/np.log(e[n]/e[n-1]) for n in range(1, len(e)-1, 1)]\n",
    "    return np.array(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXvG0ErPABZ0"
   },
   "source": [
    "Calculons le taux de convergence qui est définie par :\n",
    "$$ \\lim\\limits_{x \\rightarrow +\\infty} \\frac{|x_{k+1}-L|}{|x_{k}-L|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "CzEeCi4NAE88",
    "outputId": "08a5b556-e1e7-4ee5-bde3-c3e0c676bd05"
   },
   "outputs": [],
   "source": [
    "sigma=0.1\n",
    "nit=20000\n",
    "(alpha,xtilde,CF)=RestoreX(A,y,sigma,nit)\n",
    "L=CF[-1]\n",
    "Taux_conv=Conv_rate(CF,CF[-1])\n",
    "plt.plot(Taux_conv)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGHDIDWBsFC_"
   },
   "source": [
    "## Taux de convergence observé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "uuDRYLcOAFkE",
    "outputId": "e2520cdf-d115-46f3-d80e-cd7d28990723"
   },
   "outputs": [],
   "source": [
    "sigma=0.01\n",
    "nit=1000\n",
    "(alpha,xtilde,CF)=RestoreX(A,y,sigma,nit)\n",
    "x=np.arange(0,len(CF))\n",
    "coefficients = np.polyfit(x,np.log(CF),1) \n",
    "polynomial = np.poly1d(coefficients)\n",
    "ys = polynomial(x)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "plt.plot(ys, label=\"Pente\")\n",
    "plt.semilogy(np.log(CF), label= \"Fonction coût (log)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"La courbe semble suivre la fonction\",polynomial)\n",
    "print(\"Le taux de convergence obtenu est de\", \"{:.1e}\".format(np.abs(coefficients[0]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "VCfvOdWzAI27",
    "outputId": "a260d982-f56a-49ac-d741-0158d4bdb493"
   },
   "outputs": [],
   "source": [
    "# Taux de convergence \n",
    "sigma = 0.01\n",
    "nit = 1000\n",
    "(alpha,xtilde,CF)=RestoreX(A,y,sigma,nit)\n",
    "coefficients = np.polyfit(np.log(CF[0:nit-1]),np.log(CF[1:nit]),1) # Pente de la courbe\n",
    "polynomial = np.poly1d(coefficients)\n",
    "ys = polynomial(x)\n",
    "print(coefficients)\n",
    "print(polynomial)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "plt.loglog(np.log(CF[0:nit-1]),np.log(CF[1:nit]), label = \"Fonction Coût en log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Le taux de Convergence obtenu est de\", \"{:}\".format(np.abs(coefficients[0]*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6wjKkGoAMFg"
   },
   "source": [
    "### Interprétation taux de convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBydYL-5IKr4"
   },
   "source": [
    "**Q10.** A partir de combien de mesures pouvez-vous reconstruire exactement le signal $x$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vUDojc7ockFf",
    "outputId": "0c894dd9-2783-4a25-ed3b-70bc1bf72297"
   },
   "outputs": [],
   "source": [
    "sigma = 0.01\n",
    "nit = 5000\n",
    "for i in [5,10,15,20,25,30,40,50,60,70]:\n",
    "    p = i*4\n",
    "    A = np.random.randn(p,n) #La matrice de mesure\n",
    "    y = A.dot(x0)   #Les mesures\n",
    "    fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "    fig.suptitle(\"p =\"+\"{:}\".format(p), fontsize=25)\n",
    "\n",
    "    (alpha,xtilde,CF) = RestoreX(A,y,sigma,nit)\n",
    "\n",
    "    k = np.shape(np.where(np.abs(alpha)!=0))[1]\n",
    "    SHAN = 20*k*np.log(n)\n",
    "    print(\"________________________________\\n\\n\")\n",
    "    print(\"Constante de Shannon =\",SHAN)\n",
    "    ax[0].plot(t,Psi(alpha))\n",
    "    ax[0].set_title(\"Signal reconstruit\",y = -0.2)\n",
    "\n",
    "    ax[1].plot(t,x0,'b')\n",
    "    ax[1].set_title(\"Signal initial\",y = -0.2)\n",
    "\n",
    "    ax[2].plot(t,np.abs(x0-Psi(alpha))/np.abs(x0),'y')\n",
    "    ax[2].set_title(\"Erreur de reconstruction\",y = -0.2)\n",
    "\n",
    "    print(\"Erreur = \",npl.norm(x0-Psi(alpha))/npl.norm(x0))\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HW7iJ9wdIKr5"
   },
   "source": [
    "### Comparaison avec le théorème de Shannon \n",
    "\n",
    "On remarque que lorsque p est trop petit , donc inférieur à 120 on a de très grosses erreurs sur la reconstruction du signal\n",
    "\n",
    "Et plus p est grand, meilleur sera la reconstruction, cependant on tournera toujours autour de 0.1 en terme d'erreur.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iG3-eDZIKsF"
   },
   "source": [
    "### 4.2. Implémentation de l'itération proximale accelérée\n",
    "\n",
    "On n'a a aucun moment utilisé la convexité de la fonction $J$ pour définir l'algorithme proximal. Celui-ci est de fait sous-optimal et peut être nettement accéléré. Dans notre cas, l'algorithme accéléré (Nesterov 2007) suit le schéma suivant:\n",
    "\n",
    "Paramètres en entrée:\n",
    "* $N$ le nombre d'itérations\n",
    "* $\\alpha^0\\in \\mathbb{R}^m$ un point initial.\n",
    "\n",
    "Algorithme\n",
    "> Poser $B^0=0_\\mathbb{R}$, $g^0=0_{\\mathbb{R}^m}$, $\\alpha=\\alpha^0$\n",
    "\n",
    "> For $k$= $0$ to $N$\n",
    "\n",
    ">> $t= \\frac{2}{L}$ \n",
    "\n",
    ">> $a^k = \\frac{1}{2}\\left(t+\\sqrt{t^2+4t B^k}\\right)$ \n",
    "\n",
    ">> $v^{k} =\\mbox{prox}_{B^k\\|\\cdot\\|_1}(\\alpha^0-g^k)$ \n",
    "\n",
    ">> $w^k = \\frac{B^k \\alpha^k +a^k v^k}{B^k+a^k}$ \n",
    "\n",
    ">>  $\\alpha^{k+1} = \\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(w^k-\\frac{\\nabla J(w^k)}{L}\\right)$  \n",
    "\n",
    ">>\t$g^{k+1} = g^k + a^k\\nabla J(\\alpha^{k+1})$ \n",
    "\n",
    ">>\t$A^{k+1} = B^k+a^k$\n",
    " \n",
    "**Q11.** En vous aidant de ce que vous avez codé dans la partie précédente, implémentez cet algorithme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvPlf5lnIKsF"
   },
   "outputs": [],
   "source": [
    "def Nesterov(A,y,sigma,nit):\n",
    "    n = np.shape(A)[1]\n",
    "    x = np.zeros(nit*n) \n",
    "    alpha = np.zeros(2*n)\n",
    "    CF = np.zeros(nit)\n",
    "    B = 0\n",
    "    g = np.zeros(2*n)\n",
    "    iter = 0\n",
    "    while iter<nit:\n",
    "        L = 2*sigma*npl.norm(A)**2\n",
    "        t = 2/L\n",
    "        a = 1/2*(t+np.sqrt(t**2+4*t*B))\n",
    "        v = prox(alpha-g,B)\n",
    "        w = (B*alpha+a*v)/(B+a)\n",
    "        alpha = prox(w-1/L*PsiT(A.T.dot(A.dot(Psi(w))-y)),1/L)\n",
    "        x[n*iter:n*(iter+1)] = Psi(alpha)\n",
    "        g = g+a*PsiT(A.T.dot(A.dot(Psi(alpha))-y))\n",
    "        B += a\n",
    "        CF[iter] = npl.norm(alpha,1)+(sigma/2)*npl.norm(A.dot(Psi(alpha))-y)**2\n",
    "        iter += 1\n",
    "    return (alpha,x,CF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9t2FIT4IKsL"
   },
   "source": [
    "**Q12.** Testez le et comparez la rapidité d'execution de l'algorithme précédent et de celui-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8E01ng2TIKsO",
    "outputId": "9e2ce25d-f1ac-41e8-8a36-9c5285baf998"
   },
   "outputs": [],
   "source": [
    "nit = 10000\n",
    "SIG = [0.05,0.1,0.15,1,10,100]\n",
    "for sigma in SIG:\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    ax[0].grid()\n",
    "    ax[1].grid()\n",
    "    (alpha,xtilde,CF) = RestoreX(A,y,sigma,nit)\n",
    "    (alpha1,x,CF1) = Nesterov(A,y,sigma,nit)\n",
    "    ax[0].plot(CF)\n",
    "    ax[1].plot(CF1)\n",
    "    ax[0].yaxis.set_major_locator(FixedLocator([min(CF), max(CF)]))\n",
    "    ax[1].yaxis.set_major_locator(FixedLocator([min(CF1), max(CF1)]))\n",
    "    ax[0].axhline(y = min(CF),color = 'r',linewidth = 0.5)\n",
    "    ax[1].axhline(y = min(CF1),color = 'r',linewidth = 0.5)\n",
    "    ax[0].set_title(r\" Algo Proximal avec $\\sigma$ = \"+ \"{:.1e}\".format(sigma) + \" / nit =\" + \"{:}\".format(nit))\n",
    "\n",
    "    ax[1].set_title(r\"Nesterov avec $\\sigma$ = \"+ \"{:.1e}\".format(sigma) + \" / nit =\" + \"{:}\".format(nit))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTiRBUyHAzI0"
   },
   "source": [
    "On remarque que quelque soit la valeur de $\\sigma$ l'algorithme de Nesterov  converge plus rapidement et que pour les valeurs de $\\sigma$ considérées le minimum de l'algorithme proximal tourne autour des mêmes valeurs (entre $7$ et $8.5$). Cette valeur minimale est donc plus controlée pour Nesterov contrairement à l'algorithme du proximal  pour lequel les valeurs de ce minimum augmentent avec les valeurs de $\\sigma$.\n",
    "\n",
    "Nous allons par les suites tester l'hypothèse de Shannon concernant le nombre de mesures. Pour ce faire, nous fixons $\\sigma$ à $0.1$ et $nit$ à $20000$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Np1ALjBFVs-i",
    "outputId": "6d9be17f-31da-4a9a-cda4-c7824e4fc359"
   },
   "outputs": [],
   "source": [
    "nit = 20000\n",
    "sigma = 0.1\n",
    "for i in [5,10,15,20,25,30,40,50,60,70]:\n",
    "    p = i*4\n",
    "    A = np.random.randn(p,n) #La matrice de mesure\n",
    "    y = A.dot(x0)   #Les mesures\n",
    "    fig,ax = plt.subplots(1,3,figsize = (15,5))\n",
    "    fig.suptitle(\"p = \"+\"{:}\".format(p), fontsize=25)\n",
    "\n",
    "    (alpha,xtilde,CF) = RestoreX(A,y,sigma,nit)\n",
    "\n",
    "    k=np.shape(np.where(np.abs(alpha) != 0))[1]\n",
    "    SHAN=20*k*np.log(n)\n",
    "    print(\"________________________________\\n\\n\")\n",
    "    print(\"Constante de Shannon = \",SHAN)\n",
    "    ax[0].plot(t,Psi(alpha))\n",
    "    ax[0].set_title(\"Signal reconstruit\",y=-0.2)\n",
    "\n",
    "    ax[1].plot(t,x0,'b')\n",
    "    ax[1].set_title(\"Signal initial\",y = -0.2)\n",
    "\n",
    "    ax[2].plot(t,np.abs(x0-Psi(alpha))/np.abs(x0),'y')\n",
    "    ax[2].set_title(\"Erreur de reconstruction\",y = -0.2)\n",
    "\n",
    "    print(\"Erreur = \",npl.norm(x0-Psi(alpha))/npl.norm(x0))\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQ1CK4cPA7Vy"
   },
   "source": [
    "Nous pouvons déjà constaté que les résultats sont bien meilleures pour l'algorithme proximal. Pour $p=20$, l'erreur est de 0.84 (d'ordre 1e-1).Cela est visible au niveau du signal obtenu pour cette valeur de $p$. Nous voyons que le signal reconstruit est très différent du signal original. Cette erreur est élevée mais diminue très vite. En effet, nous constatons que nous arrivons à $0.01$ pour $p=20$ et à $0.08$ pour $p=60$. Nous arrivons au final à $0.0015$ pour $p=280$. Ces erreurs sont relativement faibles.Nous pouvons donc dire qu'on arrive assez bien à reconstruire le signal à partir de $p=40$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP2_CompressiveSensing_VFinal (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
